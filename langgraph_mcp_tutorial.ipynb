{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1aea3cb",
   "metadata": {},
   "source": [
    "# 🚀 Building LangGraph with MCP Tools: A Complete Tutorial\n",
    "\n",
    "This notebook demonstrates how to build a **multi-agent RAG (Retrieval-Augmented Generation)** system that combines:\n",
    "\n",
    "- **Local nodes** for log retrieval and processing  \n",
    "- **MCP (Model Context Protocol) agent** for metrics analysis  \n",
    "\n",
    "---\n",
    "\n",
    "## 📋 What You'll Learn\n",
    "\n",
    "1. How to create MCP servers with custom tools  \n",
    "2. How to integrate MCP tools into LangGraph nodes  \n",
    "3. How to build a hybrid pipeline with both local and MCP-powered nodes  \n",
    "4. How ReAct agents work with MCP tools  \n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Use Case\n",
    "\n",
    "Analyze **OAuth2 microservice logs** and correlate **errors with system metrics** to find root causes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab6a267e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import asyncio\n",
    "from contextlib import AsyncExitStack\n",
    "from typing_extensions import TypedDict\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# MCP (Model Context Protocol) imports\n",
    "from mcp import StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "from mcp.client.session import ClientSession\n",
    "from langchain_mcp_adapters.tools import load_mcp_tools\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# LangGraph imports\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "print(\"✅ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537101ba",
   "metadata": {},
   "source": [
    "## 🏗️ Part 2: Define Graph State\n",
    "\n",
    "The `GraphState` is the data structure that flows through all nodes in your **LangGraph**.  \n",
    "Think of it as a shared context that each node can read from and write to.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b3cffa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GraphState defined!\n"
     ]
    }
   ],
   "source": [
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    The state that flows through our LangGraph pipeline.\n",
    "    \n",
    "    Each node can read from and update this state.\n",
    "    \"\"\"\n",
    "    # Input\n",
    "    path: str                    # Path to log file\n",
    "    question: str                # User's question (can be enriched by nodes)\n",
    "    metrics_csv_path: str        # Path to metrics CSV\n",
    "    \n",
    "    # Processing\n",
    "    documents: list[str]         # Retrieved documents\n",
    "    metrics_analysis: str\n",
    "    iterations: int              # Number of query transformations\n",
    "    \n",
    "    # Output\n",
    "    generation: str              # Final answer from LLM\n",
    "    \n",
    "    # Optional metadata\n",
    "    target_service: str          # Which microservice to analyze\n",
    "\n",
    "print(\"✅ GraphState defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf5840b",
   "metadata": {},
   "source": [
    "## 🔧 Part 3: MCP Server Setup\n",
    "\n",
    "**MCP (Model Context Protocol)** allows you to create custom tools that the LLM can use.  \n",
    "We'll create a **metrics analysis server** with tools like:\n",
    "\n",
    "- `get_go_memory_stats` — Check memory usage  \n",
    "- `get_cpu_stats` — Check CPU usage  \n",
    "- `diagnose_error_correlation` — Link errors to metrics  \n",
    "\n",
    "---\n",
    "\n",
    "### Example MCP Server (see `mcp_server_metrics.py` for the full implementation)\n",
    "\n",
    "```python\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "import pandas as pd\n",
    "\n",
    "mcp = FastMCP(\"Metrics Analyzer\")\n",
    "\n",
    "@mcp.tool()\n",
    "async def get_go_memory_stats(csv_path: str) -> dict:\n",
    "    \"\"\"Get Go memory statistics from metrics CSV\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # ... analyze memory metrics\n",
    "    return {\"memory_usage\": \"85%\", \"status\": \"high\"}\n",
    "\n",
    "@mcp.tool()\n",
    "async def diagnose_error_correlation(error_message: str, csv_path: str) -> str:\n",
    "    \"\"\"Correlate errors with metrics anomalies\"\"\"\n",
    "    # ... analyze correlations\n",
    "    return \"High memory usage detected during error period\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mcp.run(transport=\"stdio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beace9ec",
   "metadata": {},
   "source": [
    "## 🤖 Part 4: Create MCP Manager\n",
    "\n",
    "The **MCP Manager** handles the lifecycle of the MCP connection:\n",
    "\n",
    "- Starting the MCP server process  \n",
    "- Loading tools from the server  \n",
    "- Cleaning up when done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c661afe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MCPManager class defined!\n"
     ]
    }
   ],
   "source": [
    "class MCPManager:\n",
    "    \"\"\"\n",
    "    Manages the lifecycle of an MCP (Model Context Protocol) server connection.\n",
    "    \n",
    "    The MCP server runs as a separate process and provides tools\n",
    "    that the LLM can use through the Model Context Protocol.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.session = None\n",
    "        self.exit_stack = None\n",
    "        self.tools = None\n",
    "    \n",
    "    async def setup(self, mcp_config):\n",
    "        \"\"\"\n",
    "        Initialize MCP connection and load tools.\n",
    "        \n",
    "        Args:\n",
    "            mcp_config: Dict with 'command' and 'args' for starting MCP server\n",
    "        \n",
    "        Returns:\n",
    "            List of LangChain tools loaded from the MCP server\n",
    "        \"\"\"\n",
    "        # Configure how to start the MCP server process\n",
    "        server_params = StdioServerParameters(\n",
    "            command=mcp_config.get(\"command\", \"./.venv/bin/python\"),\n",
    "            args=mcp_config.get(\"args\", [\"mcp_server_metrics.py\"]),\n",
    "        )\n",
    "        \n",
    "        # Use AsyncExitStack to manage async context managers\n",
    "        self.exit_stack = AsyncExitStack()\n",
    "        \n",
    "        # Start the MCP server and get read/write streams\n",
    "        read, write = await self.exit_stack.enter_async_context(\n",
    "            stdio_client(server_params)\n",
    "        )\n",
    "        \n",
    "        # Create a session to communicate with the MCP server\n",
    "        self.session = await self.exit_stack.enter_async_context(\n",
    "            ClientSession(read, write)\n",
    "        )\n",
    "        \n",
    "        # Initialize the session\n",
    "        await self.session.initialize()\n",
    "        \n",
    "        # Load tools from the MCP server\n",
    "        # These become LangChain tools that can be used by agents\n",
    "        self.tools = await load_mcp_tools(self.session)\n",
    "        \n",
    "        print(f\"✅ Loaded {len(self.tools)} tools from MCP server:\")\n",
    "        for tool in self.tools:\n",
    "            print(f\"   • {tool.name}\")\n",
    "        \n",
    "        return self.tools\n",
    "    \n",
    "    async def cleanup(self):\n",
    "        \"\"\"Close MCP connection and cleanup resources.\"\"\"\n",
    "        if self.exit_stack:\n",
    "            await self.exit_stack.aclose()\n",
    "            print(\"✅ MCP connection closed\")\n",
    "\n",
    "print(\"✅ MCPManager class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2361a7da",
   "metadata": {},
   "source": [
    "## 🧠 Part 5: Create MCP Agent Node\n",
    "\n",
    "This is where the magic happens!  \n",
    "We create a **ReAct agent** that:\n",
    "\n",
    "1. Receives a question about logs/errors  \n",
    "2. Uses MCP tools to analyze metrics  \n",
    "3. Provides a detailed metrics-based analysis to enrich the response \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2eb53a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MCPNode class defined!\n"
     ]
    }
   ],
   "source": [
    "# System prompt for the MCP agent\n",
    "SYSTEM_MESSAGE_TEMPLATE = \"\"\"You are an expert log and metrics analyst for OAuth2 microservices.\n",
    "\n",
    "You have access to:\n",
    "1. **Log Analysis**: Document retrieval from vector database (logs are pre-loaded)\n",
    "2. **Metrics Analysis**: Real-time metrics tools from MCP server\n",
    "\n",
    "WORKFLOW:\n",
    "1. First, understand WHAT errors occurred from the logs\n",
    "2. Then, check metrics to understand WHY (root cause)\n",
    "3. Finally, synthesize findings into actionable insights\n",
    "\n",
    "METRICS CONTEXT:\n",
    "- Current metrics CSV: {metrics_csv_path}\n",
    "- You can check: memory, GC, goroutines, CPU, system resources\n",
    "- Use diagnose_error_correlation to link errors to performance issues\n",
    "\n",
    "EXAMPLE WORKFLOW:\n",
    "User: \"Why did update_password fail with 404?\"\n",
    "1. Check metrics: get_go_memory_stats, get_go_gc_stats\n",
    "2. Correlate: diagnose_error_correlation(\"404 user not found\")\n",
    "3. Conclude: \"Memory pressure → slow queries → timeouts → 404s\"\n",
    "\n",
    "BE PROACTIVE: When you see errors, always check relevant metrics without being asked!\n",
    "\n",
    "Output a 2-3 sentence summary of your findings.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class MCPNode:\n",
    "    \"\"\"\n",
    "    A LangGraph node that uses a ReAct agent with MCP tools.\n",
    "    \n",
    "    This node analyzes metrics and enriches the question with context\n",
    "    that will be used by downstream nodes.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, mcp_tools, model):\n",
    "        \"\"\"\n",
    "        Initialize the MCP node with tools and model.\n",
    "        \n",
    "        Args:\n",
    "            mcp_tools: List of MCP tools loaded from the server\n",
    "            model: LangChain LLM (e.g., ChatAnthropic)\n",
    "        \"\"\"\n",
    "        self.mcp_tools = mcp_tools\n",
    "        self.model = model\n",
    "        \n",
    "        # Create a ReAct agent that can use MCP tools\n",
    "        # ReAct = Reasoning + Acting pattern\n",
    "        # The agent will: Think → Act (use tool) → Observe → Repeat\n",
    "        self.mcp_agent = create_react_agent(\n",
    "            model=self.model,\n",
    "            tools=self.mcp_tools,\n",
    "            checkpointer=MemorySaver()  # Remember conversation history\n",
    "        )\n",
    "        \n",
    "    async def mcp_agent_node(self, state: GraphState) -> dict:\n",
    "        \"\"\"\n",
    "        The actual node function that processes the state.\n",
    "        \n",
    "        Args:\n",
    "            state: Current GraphState\n",
    "            \n",
    "        Returns:\n",
    "            Dict with updated state fields\n",
    "        \"\"\"\n",
    "        print(\"🔍 --- MCP AGENT NODE ---\")\n",
    "        \n",
    "        # Extract information from state\n",
    "        question = state[\"question\"]\n",
    "        metrics_csv_path = state[\"metrics_csv_path\"]\n",
    "        documents = state[\"documents\"]\n",
    "        \n",
    "        # Format the system message with dynamic context\n",
    "        system_message_content = SYSTEM_MESSAGE_TEMPLATE.format(\n",
    "            metrics_csv_path=metrics_csv_path\n",
    "        )\n",
    "        \n",
    "        # Build context from retrieved log documents\n",
    "        log_context = \"\"\n",
    "        if documents:\n",
    "            # Extract content from Document objects properly\n",
    "            log_entries = []\n",
    "            for doc in documents:\n",
    "                if hasattr(doc, 'page_content'):\n",
    "                    content = doc.page_content\n",
    "                else:\n",
    "                    content = str(doc)\n",
    "                log_entries.append(content)\n",
    "            \n",
    "            log_context = \"\\n\\nRelevant logs:\\n\" + \"\\n---\\n\".join(log_entries)\n",
    "        \n",
    "        # Construct the full prompt for the agent\n",
    "        agent_prompt = f\"\"\"{system_message_content}\n",
    "        User question: {question}{log_context}\n",
    "        Analyze the metrics and provide a brief summary of key findings.\"\"\"\n",
    "        \n",
    "        # Run the ReAct agent\n",
    "        # The agent will autonomously decide which tools to call\n",
    "        result = await self.mcp_agent.ainvoke(\n",
    "            {\"messages\": [HumanMessage(content=agent_prompt)]},\n",
    "            config={\"configurable\": {\"thread_id\": \"mcp_agent\"}}\n",
    "        )\n",
    "        \n",
    "        # Log which tools the agent used\n",
    "        print(\"\\n🔧 Tools used by agent:\")\n",
    "        for msg in result[\"messages\"]:\n",
    "            if hasattr(msg, \"tool_calls\") and msg.tool_calls:\n",
    "                for tool_call in msg.tool_calls:\n",
    "                    print(f\"   → {tool_call['name']}\")\n",
    "        \n",
    "        # Extract the agent's analysis\n",
    "        metrics_analysis = result[\"messages\"][-1].content\n",
    "        \n",
    "        # Return updated state\n",
    "        # This allows downstream nodes to use the metrics analysis insights\n",
    "        return {\n",
    "            \"metrics_analysis\": metrics_analysis\n",
    "        }\n",
    "\n",
    "print(\"✅ MCPNode class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f192216",
   "metadata": {},
   "source": [
    "## 🏗️ Part 6: Build the Complete LangGraph\n",
    "\n",
    "Now we combine everything into a graph with multiple nodes. In the NVIDIA Nemotron Log Analysis tutorial, we already built the LangGraph nodes and edges, so in this tutorial, we’ll simply import them without re-explaining the details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9def2698",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/m/home/home2/25/phana3/data/Documents/NVIDIA_project/GenerativeAIExamples/community/log_analysis_multi_agent_rag/utils.py:4: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from binary_score_models import GradeAnswer,GradeDocuments,GradeHallucinations\n",
      "/u/25/phana3/unix/Documents/NVIDIA_project/GenerativeAIExamples/community/log_analysis_multi_agent_rag/.venv/lib/python3.12/site-packages/langchain_nvidia_ai_endpoints/_common.py:212: UserWarning: Found deepseek-ai/deepseek-v3.1 in available_models, but type is unknown and inference may fail.\n",
      "  warnings.warn(\n",
      "/u/25/phana3/unix/Documents/NVIDIA_project/GenerativeAIExamples/community/log_analysis_multi_agent_rag/.venv/lib/python3.12/site-packages/langchain_nvidia_ai_endpoints/chat_models.py:814: UserWarning: Model 'deepseek-ai/deepseek-v3.1' is not known to support structured output. Your output may fail at inference time.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from graphnodes import Nodes\n",
    "from graphedges import Edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3365a925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Graph building function defined!\n"
     ]
    }
   ],
   "source": [
    "async def build_graph(mcp_manager: MCPManager, mcp_config: dict):\n",
    "    \"\"\"\n",
    "    Build the complete LangGraph with MCP integration.\n",
    "    \n",
    "    Args:\n",
    "        mcp_manager: MCPManager instance\n",
    "        mcp_config: Configuration for MCP server\n",
    "        \n",
    "    Returns:\n",
    "        Compiled LangGraph application\n",
    "    \"\"\"\n",
    "    print(\"🏗️ Building LangGraph...\")\n",
    "    \n",
    "    # Step 1: Setup MCP and load tools\n",
    "    mcp_tools = await mcp_manager.setup(mcp_config)\n",
    "    \n",
    "    # Step 2: Create the LLM\n",
    "    model = ChatNVIDIA(\n",
    "        model=\"nvidia/llama-3.3-nemotron-super-49b-v1.5\",\n",
    "        api_key=os.getenv(\"API_KEY\"),  # Same key you're using\n",
    "        temperature=0,\n",
    "        max_tokens=20000\n",
    "    )\n",
    "    \n",
    "    # Step 3: Create the MCP agent node\n",
    "    metrics_node = MCPNode(mcp_tools=mcp_tools, model=model)\n",
    "    \n",
    "    # Step 4: Create the graph\n",
    "    graph = StateGraph(GraphState)\n",
    "    \n",
    "    # Step 5: Add all nodes\n",
    "    graph.add_node(\"retrieve\", Nodes.retrieve)\n",
    "    graph.add_node(\"rerank\", Nodes.rerank)\n",
    "    graph.add_node(\"grade_documents\", Nodes.grade_documents)\n",
    "    graph.add_node(\"generate\", Nodes.generate)\n",
    "    graph.add_node(\"transform_query\", Nodes.transform_query)\n",
    "    graph.add_node(\"mcp_agent\", metrics_node.mcp_agent_node)  # MCP-powered node!\n",
    "    \n",
    "    # Step 6: Define the flow\n",
    "    graph.add_edge(START, \"retrieve\")\n",
    "    graph.add_edge(\"retrieve\", \"rerank\")\n",
    "    graph.add_edge(\"rerank\", \"grade_documents\")\n",
    "    \n",
    "    # Conditional: Generate or transform query?\n",
    "    graph.add_conditional_edges(\n",
    "        \"grade_documents\",\n",
    "        Edge.decide_to_generate,\n",
    "        {\n",
    "            \"transform_query\": \"transform_query\",\n",
    "            \"generate\": \"mcp_agent\",  # ← Go to MCP agent before generation!\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    graph.add_edge(\"transform_query\", \"retrieve\")\n",
    "    graph.add_edge(\"mcp_agent\", \"generate\")  # ← MCP enriches question for generation\n",
    "    \n",
    "    # Final quality check\n",
    "    graph.add_conditional_edges(\n",
    "        \"generate\",\n",
    "        Edge.grade_generation_vs_documents_and_question,\n",
    "        {\n",
    "            \"not supported\": \"transform_query\",\n",
    "            \"useful\": END,\n",
    "            \"not useful\": \"transform_query\",\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    # Step 7: Compile the graph\n",
    "    print(\"✅ Graph built successfully!\")\n",
    "    return graph.compile()\n",
    "\n",
    "print(\"✅ Graph building function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe626ff",
   "metadata": {},
   "source": [
    "## 🚀 Part 7: Run the Complete Pipeline\n",
    "\n",
    "Now let's put everything together and run the pipeline!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86a1b7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "🚀 LANGGRAPH WITH MCP TUTORIAL - EXECUTION\n",
      "============================================================\n",
      "🏗️ Building LangGraph...\n",
      "✅ Loaded 8 tools from MCP server:\n",
      "   • get_go_memory_stats\n",
      "   • get_go_gc_stats\n",
      "   • get_goroutine_stats\n",
      "   • get_system_memory_stats\n",
      "   • get_cpu_stats\n",
      "   • detect_anomalies\n",
      "   • diagnose_error_correlation\n",
      "   • get_metrics_summary\n",
      "✅ Graph built successfully!\n",
      "\n",
      "📝 Initial Question:\n",
      "   What are the critical errors in the log file? Summarize the metrics analysis, and explain how the metrics correlate with potential issues\n",
      "\n",
      "🏃 Running graph pipeline...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/25/phana3/unix/Documents/NVIDIA_project/GenerativeAIExamples/community/log_analysis_multi_agent_rag/.venv/lib/python3.12/site-packages/langchain_nvidia_ai_endpoints/_common.py:212: UserWarning: Found nvidia/llama-3.3-nemotron-super-49b-v1.5 in available_models, but type is unknown and inference may fail.\n",
      "  warnings.warn(\n",
      "/u/25/phana3/unix/Documents/NVIDIA_project/GenerativeAIExamples/community/log_analysis_multi_agent_rag/.venv/lib/python3.12/site-packages/langchain_nvidia_ai_endpoints/chat_models.py:591: UserWarning: Model 'nvidia/llama-3.3-nemotron-super-49b-v1.5' is not known to support tools. Your tool binding may fail at inference time.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE---\n",
      "NVIDIA--RERANKER\n",
      "CHECKING DOCUMENT RELEVANCE TO QUESTION\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "ASSESS GRADED DOCUMENTS\n",
      "---DECISION: GENERATE---\n",
      "🔍 --- MCP AGENT NODE ---\n",
      "\n",
      "🔧 Tools used by agent:\n",
      "   → diagnose_error_correlation\n",
      "GENERATE USING LLM\n",
      "GRADE GENERATED vs QUESTION\n",
      "DECISION: GENERATION ADDRESSES QUESTION\n",
      "\n",
      "============================================================\n",
      "✅ FINAL RESULT\n",
      "============================================================\n",
      "\n",
      "📄 Generation:\n",
      "Based on my analysis of the provided log files, here is a comprehensive breakdown of the critical errors and their correlation with system metrics:\n",
      "\n",
      "### Summary\n",
      "The log files reveal a critical pattern of **500 Internal Server Errors** occurring during `PUT /oauth2/client` requests. These errors are caused by `NullPointerException: Null key is not allowed!` originating from Hazelcast map operations. The errors consistently occur during client registration/update operations and result in runtime exceptions that halt request processing.\n",
      "\n",
      "### Key Issues\n",
      "1. **Null Pointer Exceptions in Hazelcast Operations**: Multiple instances of `NullPointerException` occur when attempting to perform `Map.get()` operations with null keys.\n",
      "2. **Failed Client Data Validation**: Errors occur during JSON schema validation of client data payloads, though validation appears to complete successfully before the Hazelcast error.\n",
      "3. **Runtime Exception Handling**: The system converts `NullPointerException` into generic runtime exceptions with error code `ERR10010`, masking the specific root cause.\n",
      "\n",
      "### Error Details\n",
      "**Primary Error:**\n",
      "- **Error Type**: `NullPointerException`\n",
      "- **Error Message**: `\"Null key is not allowed!\"`\n",
      "- **Location**: `com.hazelcast.map.impl.proxy.MapProxyImpl.get(MapProxyImpl.java:118)`\n",
      "- **Timestamps**: Multiple occurrences between 18:13:29.078 - 18:13:29.460\n",
      "- **HTTP Status**: 500 Internal Server Error\n",
      "- **Error Code**: ERR10010 (RUNTIME_EXCEPTION)\n",
      "\n",
      "**Stack Trace Pattern:**\n",
      "The errors follow a consistent pattern through the handler chain:\n",
      "1. Request enters through OpenAPI validation handlers\n",
      "2. Passes through Audit, Body, Correlation handlers\n",
      "3. Fails at `Oauth2ClientPutHandler.handleRequest()` \n",
      "4. Triggers Hazelcast `MapProxyImpl.get()` with null key\n",
      "5. ExceptionHandler converts to generic runtime exception\n",
      "\n",
      "**Affected Endpoints:**\n",
      "- `PUT /oauth2/client` (multiple client IDs affected)\n",
      "- All failing requests contain client data payloads with seemingly valid structure\n",
      "\n",
      "### Metrics Correlation Analysis\n",
      "Based on the log data and the nature of the errors, here is how metrics would correlate with these issues:\n",
      "\n",
      "1. **Memory Metrics**: \n",
      "   - Normal memory usage patterns expected (no OOM errors in logs)\n",
      "   - However, null key issues could indicate memory corruption or improper object initialization\n",
      "\n",
      "2. **Garbage Collection**:\n",
      "   - No GC-related issues evident in logs\n",
      "   - The null pointer exceptions are logic errors, not memory pressure issues\n",
      "\n",
      "3. **CPU Utilization**:\n",
      "   - Errors occur rapidly in sequence (multiple within milliseconds)\n",
      "   - This suggests adequate CPU resources but potential race conditions\n",
      "\n",
      "4. **Goroutine/Thread Analysis**:\n",
      "   - All errors occur in `XNIO-1 task-1` thread\n",
      "   - No thread exhaustion evident, but single-thread pattern might indicate inefficient locking\n",
      "\n",
      "5. **Request Rate & Error Rate**:\n",
      "   - High error concentration within short timeframe (~400ms window)\n",
      "   - This suggests either:\n",
      "     - A burst of invalid requests\n",
      "     - A systemic issue affecting all client operations\n",
      "\n",
      "6. **Hazelcast Metrics**:\n",
      "   - (Not visible in logs) Would show map operation failure rates\n",
      "   - Potential cache miss patterns or null key insertion attempts\n",
      "\n",
      "### Root Cause Assessment\n",
      "The metrics pattern suggests:\n",
      "- **Application Logic Error**: The consistent null key issue indicates a code defect rather than resource exhaustion\n",
      "- **Data Validation Gap**: JSON validation passes but business logic fails to handle null keys\n",
      "- **Timing Issue**: Rapid succession of errors might indicate race condition in key generation\n",
      "- **Cache Inconsistency**: Potential stale cache entries or improper cache key construction\n",
      "\n",
      "### Recommendations\n",
      "1. **Immediate Actions**:\n",
      "   - Add null checks in `Oauth2ClientPutHandler.handleRequest()` before Hazelcast operations\n",
      "   - Enhance logging to capture the specific key value that's causing null issues\n",
      "\n",
      "2. **Investigation Priorities**:\n",
      "   - Examine client ID generation logic\n",
      "   - Review Hazelcast configuration for null key handling\n",
      "   - Check for race conditions in client registration workflow\n",
      "\n",
      "3. **Prevention**:\n",
      "   - Implement stricter input validation before reaching Hazelcast operations\n",
      "   - Add circuit breakers for repeated failures on same endpoint\n",
      "   - Improve exception handling to preserve original error details\n",
      "\n",
      "4. **Monitoring Enhancements**:\n",
      "   - Add metrics for null key exceptions specifically\n",
      "   - Implement alerting on ERR10010 patterns\n",
      "   - Track client registration success/failure rates\n",
      "\n",
      "The errors appear to be application-level issues rather than infrastructure problems, with the primary culprit being improper null handling in client data processing before Hazelcast operations.\n",
      "\n",
      "============================================================\n",
      "🎉 Pipeline completed successfully!\n",
      "============================================================\n",
      "✅ MCP connection closed\n"
     ]
    }
   ],
   "source": [
    "async def main():\n",
    "    \"\"\"\n",
    "    Main execution function that runs the complete pipeline.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"🚀 LANGGRAPH WITH MCP TUTORIAL - EXECUTION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Step 1: Configure MCP Server\n",
    "    mcp_config = {\n",
    "        \"command\": \"./.venv/bin/python\",  # Python command\n",
    "        \"args\": [\"mcp_server_metrics.py\"],  # Your MCP server script\n",
    "    }\n",
    "    \n",
    "    # Step 2: Create MCP Manager\n",
    "    mcp_manager = MCPManager()\n",
    "    \n",
    "    try:\n",
    "        # Step 3: Build the graph\n",
    "        graph = await build_graph(\n",
    "            mcp_manager=mcp_manager,\n",
    "            mcp_config=mcp_config\n",
    "        )\n",
    "        \n",
    "        # Step 4: Prepare initial state\n",
    "        initial_state = {\n",
    "            \"path\": \"data/logs/update_password_404_user_not_found/light-oauth2-oauth2-client-1.log\",\n",
    "            \"question\": \"What are the critical errors in the log file? Summarize the metrics analysis, and explain how the metrics correlate with potential issues\",\n",
    "            \"documents\": [],\n",
    "            \"generation\": \"\",\n",
    "            \"metrics_analysis\": \"\",\n",
    "            \"iterations\": 0,\n",
    "            \"target_service\": \"oauth2-client\",\n",
    "            \"metrics_csv_path\": \"data/metrics/light-oauth2-data-1719771248.csv\"\n",
    "        }\n",
    "        \n",
    "        print(\"\\n📝 Initial Question:\")\n",
    "        print(f\"   {initial_state['question']}\")\n",
    "        \n",
    "        # Step 5: Run the graph\n",
    "        print(\"\\n🏃 Running graph pipeline...\\n\")\n",
    "        result = await graph.ainvoke(initial_state)\n",
    "        \n",
    "        # Step 6: Display results\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"✅ FINAL RESULT\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"\\n📄 Generation:\\n{result['generation']}\\n\")\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"🎉 Pipeline completed successfully!\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "    finally:\n",
    "        # Step 7: Always cleanup\n",
    "        await mcp_manager.cleanup()\n",
    "\n",
    "# Run the pipeline\n",
    "await main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "log_analysis_multi_agent_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
